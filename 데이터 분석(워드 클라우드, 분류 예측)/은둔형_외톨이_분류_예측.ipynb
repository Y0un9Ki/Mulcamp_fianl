{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0) 필요한 라이브러리 설치"
      ],
      "metadata": {
        "id": "RsSiZ7tjBWQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "dLlbtNQYBU0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) 필요한 라이브러리 import"
      ],
      "metadata": {
        "id": "QTh3GOJxBaV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # 데이터 분석 및 조작을 위한 라이브러리\n",
        "import numpy as np # 수치 계산을 위한 라이브러리\n",
        "from sklearn.model_selection import train_test_split, cross_val_score # 데이터를 학습용과 테스트용으로 분할 / 교차 검증 점수를 계산\n",
        "from sklearn.preprocessing import LabelEncoder #데이터 전처리를 위한 라벨 인코딩 라이브러리\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report# 모델 평가를 위한 평가지표 라이브러리\n",
        "from sklearn.ensemble import RandomForestClassifier, StakingClassifier # 랜덤 포레스트 분류기, 스태킹 import\n",
        "from xgboost import XGBClassifier # XGBoost 분류기\n",
        "from catboost import CatBoostClassifier # CatBoost 분류기\n",
        "import optuna # 하이퍼파라미터 최적화를 위한 라이브러리\n",
        "from optuna.samplers import TPESampler # TPESampler를 사용한 최적화\n",
        "from imblearn.over_sampling import SMOTE  # 불균형 데이터 처리\n",
        "import time # 시간 측정을 위한 라이브러리"
      ],
      "metadata": {
        "id": "HLDtbiF2BZrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 데이터 불러오기"
      ],
      "metadata": {
        "id": "HbYw3WapBnL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/파이널 프로젝트/data/서울시 고립은둔청년 실태조사(청년조사)_분류.csv'\n",
        "df = pd.read_csv(file_path, encoding = 'euc-kr')"
      ],
      "metadata": {
        "id": "vFPa4byUBkKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) 전처리"
      ],
      "metadata": {
        "id": "5aAUuLCyBpXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확인 및 전처리\n",
        "target = '【KEY_1】 고립은둔청년'\n",
        "\n",
        "\n",
        "# 종속변수를 이진 분류로 변환\n",
        "df[target] = df[target].apply(lambda x: 1 if x == '해당' else 0)"
      ],
      "metadata": {
        "id": "Lj1JlCpVBfAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a10, a11열 이상치 제거 함수 (각각 300개, 98개 제거)\n",
        "def remove_outlier(df, column):\n",
        "  q3 = df[column].quantile(0.75)\n",
        "  q1 = df[column].quantile(0.25)\n",
        "\n",
        "  IQR = q3 - q1\n",
        "  max_val = q3 + 1.5 * q3\n",
        "\n",
        "  cond = (df[column] >= 0) & (df[column] < max_val)\n",
        "  return df[cond]\n",
        "\n",
        "# 이상치 제거\n",
        "df = remove_outlier(df, '【A10】 지난 2주간 교류 상대(명)')\n",
        "df = remove_outlier(df, '【A11】 지난 2주 동안 교류 횟수(회)')"
      ],
      "metadata": {
        "id": "9eCzcP11Bs-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(target, axis = 1)\n",
        "y = df[target]"
      ],
      "metadata": {
        "id": "qWEF-57UBvRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 인코딩\n",
        "cols_object = X.select_dtypes(include = 'object').columns\n",
        "le_X = X.copy()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in cols_object:\n",
        "  le_X[col] = le.fit_transform(le_X[col])"
      ],
      "metadata": {
        "id": "6zwRwIcKBwnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) train, test 데이터 분리\n",
        "\n"
      ],
      "metadata": {
        "id": "IprLx-qzByE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_le, X_test_le, y_train, y_test = train_test_split(le_X, y, test_size=0.2, random_state=2024)"
      ],
      "metadata": {
        "id": "FzA3OefDBy33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_X_train_le, new_y_train = SMOTE(random_state = 2024).fit_resample(X_train_le, y_train)"
      ],
      "metadata": {
        "id": "4Lc8hsScCZLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) 하이퍼파라미터 튜닝"
      ],
      "metadata": {
        "id": "Pr_Bvw1kB0-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest\n",
        "def objective_rf(trial, X_train, y_train):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 32),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    }\n",
        "    rf_model = RandomForestClassifier(**params, random_state=42)\n",
        "    scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# XGBoost\n",
        "def objective_xgb(trial, X_train, y_train):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 32),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
        "    }\n",
        "    xgb_model = XGBClassifier(**params, random_state=42)\n",
        "    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# CatBoost\n",
        "def objective_cat(trial, X_train, y_train):\n",
        "    param = {\n",
        "        'iterations': trial.suggest_int('iterations', 200, 1000),\n",
        "        'depth': trial.suggest_int('depth', 4, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
        "        'border_count': trial.suggest_int('border_count', 32, 128)\n",
        "    }\n",
        "\n",
        "    cat_model = CatBoostClassifier(**param, verbose=1)\n",
        "    scores = cross_val_score(cat_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n"
      ],
      "metadata": {
        "id": "I3N5Z2pUB2cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# 랜덤 포레스트 최적화\n",
        "study_rf_le = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
        "study_rf_le.optimize(lambda trial: objective_rf(trial, new_X_train_le, new_y_train), n_trials=50)\n",
        "best_params_rf_le = study_rf_le.best_params\n",
        "print(f'Best parameters for Random Forest with Label Encoding: {best_params_rf_le}')\n",
        "\n",
        "# XGBoost 최적화\n",
        "study_xgb_le = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
        "study_xgb_le.optimize(lambda trial: objective_xgb(trial, new_X_train_le, new_y_train), n_trials=50)\n",
        "best_params_xgb_le = study_xgb_le.best_params\n",
        "print(f'Best parameters for XGBoost with Label Encoding: {best_params_xgb_le}')\n",
        "\n",
        "# CatBoost 최적화\n",
        "study_cat = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
        "study_cat.optimize(lambda trial: objective_cat(trial, new_X_train_le, new_y_train), n_trials=20)\n",
        "best_params_cat = study_cat.best_params\n",
        "print(f'Best parameters for CatBoost with Label Encoding: {best_params_cat}')"
      ],
      "metadata": {
        "id": "7E-HQkMmB-IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) 머신러닝 모델 최적화 및 스태킹 앙상블 평가\n",
        "\n"
      ],
      "metadata": {
        "id": "K9uHEZb-B62v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤포레스트 최적화 파라미터 학습 및 예측\n",
        "rf_model_le = RandomForestClassifier(**best_params_rf_le, random_state=42)\n",
        "rf_model_le.fit(new_X_train_le, new_y_train)\n",
        "y_pred_rf = rf_model_le.predict(X_test_le)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# XGBoost 최적화 파라미터 학습 및 예측\n",
        "xgb_model_le = XGBClassifier(**best_params_xgb_le, random_state=42)\n",
        "xgb_model_le.fit(new_X_train_le, new_y_train)\n",
        "y_pred_xgb = xgb_model_le.predict(X_test_le)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "# CatBoost 최적화 파라미터 학습 및 예측\n",
        "cat_model = CatBoostClassifier(**best_params_cat, random_state=42, verbose=0)\n",
        "cat_model.fit(new_X_train_le, new_y_train)\n",
        "y_pred_cat = cat_model.predict(X_test_le)\n",
        "accuracy_cat = accuracy_score(y_test, y_pred_cat)\n",
        "\n",
        "# 스태킹 모델\n",
        "estimators = [\n",
        "    ('rf_le', rf_model_le),\n",
        "    ('xgb_le', xgb_model_le),\n",
        "    ('cat', cat_model)\n",
        "]\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n",
        "stacking_model.fit(new_X_train_le, new_y_train)\n",
        "y_pred_stacking = stacking_model.predict(X_test_le)\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "precision_stacking = precision_score(y_test, y_pred_stacking)\n",
        "recall_stacking = recall_score(y_test, y_pred_stacking)\n",
        "f1_stacking = f1_score(y_test, y_pred_stacking)\n",
        "\n",
        "print(f'Random Forest Accuracy: {accuracy_rf:.4f}')\n",
        "print(f'XGBoost Accuracy: {accuracy_xgb:.4f}')\n",
        "print(f'CatBoost Accuracy: {accuracy_cat:.4f}')\n",
        "\n",
        "print(f'Stacking Model Accuracy: {accuracy_stacking:.4f}')\n",
        "print(f'Precision: {precision_stacking:.4f}')\n",
        "print(f'Recall: {recall_stacking:.4f}')\n",
        "print(f'F1 Score: {f1_stacking:.4f}')\n",
        "print(classification_report(y_test, y_pred_stacking))\n",
        "\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time:.2f} seconds')\n"
      ],
      "metadata": {
        "id": "rkXtGZIlCBwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}